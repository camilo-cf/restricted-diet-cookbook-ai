# AI Agents & Workflows

## Overview
This repository was developed using an AI-centric workflow, leveraging autonomous agents for architectural planning, code generation, and infrastructure management.

## AI Tools Used
| Tool | Purpose | Usage Location |
|Data | Definition | Context |
|---|---|---|
| **Antigravity (Google DeepMind)** | Primary Agent | Orchestrating the entire dev lifecycle, from planning to deployment. |
| **OpenAI GPT-4** | Code Intelligence | Used within the application (`apps/api/app/core/ai.py`) for recipe generation logic. |
| **Playwright** | E2E Testing Agent | Automated user journey verification in `apps/web/e2e/wizard.spec.ts`. |
| **PostgreSQL & MinIO** | Simulated Environment | Dockerized services providing realistic testbeds for AI agents. |

## Workflow Example: API Contract to Implementation
A concrete example of how AI accelerated the development loop:

1.  **Contract Definition**: 
    - AI Agent defined `openapi.yaml` to strictly specify the `WizardData` schema and `/generate` endpoint.
2.  **Stub Generation**:
    - Agent ran `pnpm gen:client` to auto-generate fully typed TypeScript fetcher clients in `packages/api-client`.
3.  **Implementation**:
    - Agent implemented the backend route `apps/api/app/api/routes/ai.py` matching the openapi spec.
    - Agent scaffolded the Frontend `WizardContext` using the generated types.
4.  **Verification**:
    - Agent wrote and ran `apps/web/e2e/wizard.spec.ts` to verify the end-to-end flow.
    - **CI Integration**: Tests run automatically on commit (via `husky` and `run.sh`).

## Validation Strategy
To ensure AI-generated code meets production standards:
1.  **Strict Typing**: Monorepo uses strict TypeScript and Pydantic models. Discrepancies break the build immediately.
2.  **Contract Tests**: `packages/api-client` ensures frontend/backend drift is impossible.
3.  **Containerized Verification**: E2E tests run inside Docker containers (`run.sh test`), ensuring the "it works on my machine" fallacy is avoided.

## AI Acceptance Rate
*   **Estimated Rate**: ~95%
*   **Context**: The vast majority of boilerplate, configuration (`docker-compose.yml`, `render.yaml`), and logic (React contexts, FastAPI routes, unit tests) was generated by the Agent. Human intervention was primarily for final environment-specific configurations during Render deployment.

---

## ðŸ”Œ Model Context Protocol (MCP)
This project was developed by an agent leveraging the **Model Context Protocol (MCP)** to interact with the host environment safely and efficiently.

### MCP Tools Used during Development:
1.  **Filesystem Server**: Managed the complex monorepo structure, ensuring imports between `packages/api-client` and `apps/web` remained consistent.
2.  **Shell/Terminal Server**: Automated the execution of `pnpm build`, `docker-compose up`, and `pytest` during the iterative "implement-and-verify" loops.
3.  **Search Server**: Used `grep_search` and `find_by_name` to maintain architectural consistency across the Python and TypeScript codebases.
4.  **Web Research Server**: Utilized MCP-powered web searching to resolve deployment issues on Render and investigate specific Next.js 14 hydration errors.

### Why MCP?
Using MCP allowed the developer agent to **bridge the gap** between LLM reasoning and the reality of a Linux terminal. This ensured that the configurations in `render.yaml` and the Dockerfiles weren't just "theoretically correct" but were verified in a live environment before being committed.
